{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os \n",
    "import time\n",
    "\n",
    "cwd = os.getcwd()\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_save_path = os.path.join(cwd, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "def get_MNIST_data():\n",
    "    \n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(root=f\"{cwd}\\data\\MNIST\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    test_dataset = torchvision.datasets.MNIST(root=f\"{cwd}\\data\\MNIST\", train=False, download=True, transform=torchvision.transforms.ToTensor()) \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_CIFAR10_data():\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=f\"{cwd}\\data\\CIFAR10\", train=True, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    testset = torchvision.datasets.CIFAR10(root=f\"{cwd}\\data\\CIFAR10\", train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "class MNIST_model1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.index = 1\n",
    "        self.conv = torch.nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = torch.nn.Linear(16 * 28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv(x))  \n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class MNIST_model2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.index = 2\n",
    "        self.conv_block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(6, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16 * 28 * 28, 84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc_block(out) \n",
    "        return out\n",
    "    \n",
    "class MNIST_model3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.index = 3\n",
    "        self.conv_block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(6),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            torch.nn.Conv2d(6, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16 * 7 * 7, 120),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(120, 84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(84, 10)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc_block(out)\n",
    "        return out\n",
    "    \n",
    "class CIFAR10_model1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.index = 4\n",
    "        self.conv = torch.nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = torch.nn.Linear(16 * 32 * 32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv(x))  \n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class CIFAR10_model2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.index = 5\n",
    "        self.conv_block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 6, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(6, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16 * 32 * 32, 84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc_block(out) \n",
    "        return out\n",
    "\n",
    "class CIFAR10_model3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.index = 6\n",
    "        self.conv_block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 6, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(6),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            torch.nn.Conv2d(6, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16 * 8 * 8, 120),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(120, 84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(84, 10)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc_block(out)\n",
    "        return out\n",
    "    \n",
    "class Bonus_CIFAR10_model1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.index = 7\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 9, kernel_size=3, padding=1),  # Smaller kernel, more channels\n",
    "            torch.nn.BatchNorm2d(9),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(9, 15, kernel_size=4, padding=1),\n",
    "            torch.nn.BatchNorm2d(15),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(15, 30, kernel_size=5, padding=1),  # Increase channels further\n",
    "            torch.nn.BatchNorm2d(30),\n",
    "            torch.nn.ReLU()\n",
    "        ) \n",
    "\n",
    "        # Calculate the flattened output size before the linear layers\n",
    "        self.flatten_size = self._calculate_flatten_size()  \n",
    "\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(self.flatten_size, 256),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10)  # Output for 10 CIFAR10 classes\n",
    "        )\n",
    "\n",
    "    def _calculate_flatten_size(self):\n",
    "        # Simulate a forward pass to get the flattened output size \n",
    "        test_input = torch.randn(1, 3, 32, 32) \n",
    "        out = self.conv_block3(self.conv_block2(self.conv_block1(test_input)))\n",
    "        return out.flatten(start_dim=1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block3(self.conv_block2(self.conv_block1(x)))\n",
    "        out = self.fc_block(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def Get_models():\n",
    "    mnist_model1 = MNIST_model1()\n",
    "    mnist_model2 = MNIST_model2()\n",
    "    mnist_model3 = MNIST_model3()\n",
    "    cifar10_model1 = CIFAR10_model1()\n",
    "    cifar10_model2 = CIFAR10_model2()\n",
    "    cifar10_model3 = CIFAR10_model3()\n",
    "    bonuscifar10_model = Bonus_CIFAR10_model1()\n",
    "\n",
    "    \n",
    "    return mnist_model1, mnist_model2, mnist_model3, cifar10_model1, cifar10_model2, cifar10_model3, bonuscifar10_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test function\n",
    "def train_model(model, optimizers, loss, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer = optimizers[model.index-1]\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            output = model(data)   # Forward pass\n",
    "            loss_val = loss(output, target)  # Calculate loss\n",
    "            loss_val.backward()        # Compute gradients\n",
    "            optimizer.step()       # Update model parameters \n",
    "\n",
    "            if batch_idx % 1000 == 0:  # Print progress\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss_val.item()))\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_path, f\"model_{model.index}.pth\"))\n",
    "            \n",
    "    \n",
    "def test_models(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mnist_train_loader, mnist_test_loader = get_MNIST_data()\n",
    "cifar10_train_loader, cifar10_test_loader = get_CIFAR10_data()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizers = []\n",
    "#Getting models\n",
    "models = Get_models()\n",
    "os.makedirs(model_save_path, exist_ok=True) \n",
    "\n",
    "for i in range(len(models)):\n",
    "    optimizers.append(torch.optim.Adam(models[i].parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Model 1 Loading...\n",
      "MNIST model 1 is not trained, Training...\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.294046\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.501836\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.080298\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.014065\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.101479\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.008246\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.002732\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.014031\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.029695\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.000324\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000420\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.021018\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.003069\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.003999\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.005720\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.031793\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000886\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.000618\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000743\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001680\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000553\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.007620\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.056179\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.000651\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.004833\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.000384\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.002081\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.150499\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.003093\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.000470\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.000045\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.000633\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.000156\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.000266\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.001132\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.000060\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.000855\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000215\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.000029\n",
      "Model 2 Loading...\n",
      "MNIST model 2 is not trained, Training...\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.300021\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.196834\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.016549\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.049660\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.085101\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.005321\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001937\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.009883\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001534\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.003808\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.016501\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.000097\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.271150\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006937\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.000199\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.004053\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.015926\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001865\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000103\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.000036\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000010\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.000078\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.000306\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.000106\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.000001\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.000371\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.000210\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.000003\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.000737\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.000938\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000003\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.000130\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.002654\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000039\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.000039\n",
      "Model 3 Loading...\n",
      "MNIST model 3 is not trained, Training...\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.280623\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.106579\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.003466\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.085994\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.046791\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.034011\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.008135\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.201159\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.002808\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.011479\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.012561\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.006329\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.023851\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.003351\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.007914\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.000362\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.002494\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.000951\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000613\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.029034\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.072209\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000788\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.010637\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.000662\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.001735\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.000021\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.049540\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.000587\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.078690\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.000029\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.000802\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.000351\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.005286\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.002099\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.001606\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.035713\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.000092\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.000049\n",
      "Model 4 Loading...\n",
      "CIFAR10 model 4 is not trained, Training...\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.323452\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.147943\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.335791\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.251660\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.903602\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.778191\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.857368\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.715882\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.830333\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.799863\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.667430\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.808630\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.526371\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.534492\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.378768\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.758759\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.731898\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.531201\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.574305\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.548271\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.347989\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.889003\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.205098\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 0.650680\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.417568\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 0.408840\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.295391\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 0.561007\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.376884\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 0.465030\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.494880\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 0.465041\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.193608\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 0.356283\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.459190\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 0.387594\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.156429\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 0.179977\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.170761\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 0.246869\n",
      "Model 5 Loading...\n",
      "CIFAR10 model 5 is not trained, Training...\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.302844\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.299448\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.145809\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.360151\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.904195\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.029030\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.585528\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.353118\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.690068\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.622756\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.579383\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.591054\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.467921\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.346343\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.086549\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.385473\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.250081\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.074508\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.157574\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.142840\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.300536\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.019997\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.063776\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 0.188034\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.040800\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 0.375616\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.097306\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 0.280213\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.045436\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 0.014743\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.108540\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 0.037129\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.041488\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 0.040327\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.062070\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 0.097937\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.003464\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 0.004891\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.048440\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 0.005303\n",
      "Model 6 Loading...\n",
      "CIFAR10 model 6 is not trained, Training...\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.324850\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.084050\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.978291\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.918106\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.952688\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.247247\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.010870\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.769629\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.978278\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.785652\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.720369\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.642792\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.995531\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.698090\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.856773\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.567021\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.843017\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.407117\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.655779\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.009172\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.891861\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.862836\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.510555\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 0.428190\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.550527\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 0.781039\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.300128\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 0.724269\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.271944\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 0.675817\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.464075\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 0.797264\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.567209\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 0.466011\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.392900\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 0.634754\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.550057\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 0.422086\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.387522\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 0.426331\n",
      "Model 7 Loading...\n",
      "Bonus CIFAR10 model 7 is not trained, Training...\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.323691\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.982552\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.047923\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.805968\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.655670\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.435912\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.566982\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.592430\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.641906\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.658162\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.649167\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.506944\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.500062\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.823393\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.418341\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.414902\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.572813\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.286181\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.645308\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.297532\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.447836\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.235746\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.267456\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 0.267857\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.189273\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 0.354267\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.376689\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 0.312483\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.116548\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 0.214158\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.138792\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 0.407628\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.438538\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 0.309893\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.121086\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 0.402423\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.147347\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 0.204373\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.045473\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 0.267589\n"
     ]
    }
   ],
   "source": [
    "#check if all modles are trained\n",
    "time_data = []\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "for model in models:\n",
    "    print(f\"Model {model.index} Loading...\")\n",
    "    if os.path.exists(os.path.join(model_save_path, f\"model_{model.index}.pth\")):\n",
    "        model.load_state_dict(torch.load(os.path.join(model_save_path, f\"model_{model.index}.pth\")))\n",
    "        continue\n",
    "    else:\n",
    "        curtime = time.time()\n",
    "        if model.index <=3:\n",
    "            print(f\"MNIST model {model.index} is not trained, Training...\")\n",
    "            train_model(model, optimizers, loss_fn, mnist_train_loader)\n",
    "        elif model.index == 7:\n",
    "            print(f\"Bonus CIFAR10 model {model.index} is not trained, Training...\")\n",
    "            train_model(model, optimizers, loss_fn, cifar10_train_loader)\n",
    "        else:\n",
    "            print(f\"CIFAR10 model {model.index} is not trained, Training...\")\n",
    "            train_model(model, optimizers, loss_fn, cifar10_train_loader)\n",
    "            \n",
    "        finish_time = time.time()\n",
    "        time_data.append(finish_time - curtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Time to train model in seconds for model 1: 245.1836473941803\n",
      "MNIST model 1 Testing...\n",
      "\n",
      "Test set: Average loss: 0.0031, Accuracy: 9822/10000 (98%)\n",
      "\n",
      "Time to train model in seconds for model 2: 572.8232760429382\n",
      "MNIST model 2 Testing...\n",
      "\n",
      "Test set: Average loss: 0.0027, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Time to train model in seconds for model 3: 305.9659881591797\n",
      "MNIST model 3 Testing...\n",
      "\n",
      "Test set: Average loss: 0.0011, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Time to train model in seconds for model 4: 323.540992975235\n",
      "CIFAR10 model 4 Testing...\n",
      "\n",
      "Test set: Average loss: 0.0705, Accuracy: 5589/10000 (56%)\n",
      "\n",
      "Time to train model in seconds for model 5: 683.2310018539429\n",
      "CIFAR10 model 5 Testing...\n",
      "\n",
      "Test set: Average loss: 0.1338, Accuracy: 5754/10000 (58%)\n",
      "\n",
      "Time to train model in seconds for model 6: 397.55051922798157\n",
      "CIFAR10 model 6 Testing...\n",
      "\n",
      "Test set: Average loss: 0.0355, Accuracy: 6703/10000 (67%)\n",
      "\n",
      "Time to train model in seconds for model 7: 879.403082370758\n",
      "Bonus CIFAR10 model 7 Testing...\n",
      "\n",
      "Test set: Average loss: 0.0373, Accuracy: 7310/10000 (73%)\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------\")\n",
    "log_file = os.path.join(model_save_path, \"time_log.txt\")\n",
    "for model in models:\n",
    "    try:\n",
    "        print(f\"Time to train model in seconds for model {model.index}: {time_data[model.index-1]}\")\n",
    "        #store time in log file\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"Time to train model in seconds for model {model.index}: {time_data[model.index-1]}\\n\")\n",
    "    except:\n",
    "        pass\n",
    "    if model.index <=3:\n",
    "        print(f\"MNIST model {model.index} Testing...\")\n",
    "        test_models(model, mnist_test_loader)\n",
    "    elif model.index == 7:\n",
    "        print(f\"Bonus CIFAR10 model {model.index} Testing...\")\n",
    "        test_models(model, cifar10_test_loader)\n",
    "    else:\n",
    "        print(f\"CIFAR10 model {model.index} Testing...\")\n",
    "        test_models(model, cifar10_test_loader)\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The model with the least amount of error for validation was MNIST model 3 with an accuracy of 99% and an Accuracy of 9874/10000\n",
    "    For the CIFAR10 model 7 with an accuracy of 73% and an Accuracy of 7282/10000\n",
    "    \n",
    "    The time to train each model was and accuracy for all of them is: \n",
    "    \n",
    "    Time to train model in seconds for model 1: 245.1836473941803\n",
    "    MNIST model 1 Testing...\n",
    "\n",
    "    Test set: Average loss: 0.0031, Accuracy: 9822/10000 (98%)\n",
    "\n",
    "    Time to train model in seconds for model 2: 572.8232760429382\n",
    "    MNIST model 2 Testing...\n",
    "\n",
    "    Test set: Average loss: 0.0027, Accuracy: 9876/10000 (99%)\n",
    "\n",
    "    Time to train model in seconds for model 3: 305.9659881591797\n",
    "    MNIST model 3 Testing...\n",
    "\n",
    "    Test set: Average loss: 0.0011, Accuracy: 9915/10000 (99%)\n",
    "\n",
    "    Time to train model in seconds for model 4: 323.540992975235\n",
    "    CIFAR10 model 4 Testing...\n",
    "\n",
    "    Test set: Average loss: 0.0705, Accuracy: 5589/10000 (56%)\n",
    "\n",
    "    Time to train model in seconds for model 5: 683.2310018539429\n",
    "    CIFAR10 model 5 Testing...\n",
    "\n",
    "    Test set: Average loss: 0.1338, Accuracy: 5754/10000 (58%)\n",
    "    ...\n",
    "\n",
    "    Test set: Average loss: 0.0373, Accuracy: 7310/10000 (73%)\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
